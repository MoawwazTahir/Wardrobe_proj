{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630},{"sourceId":9680276,"sourceType":"datasetVersion","datasetId":5916796}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers\n!pip install qwen-vl-utils","metadata":{"execution":{"iopub.status.busy":"2024-10-23T09:49:05.961756Z","iopub.execute_input":"2024-10-23T09:49:05.962125Z","iopub.status.idle":"2024-10-23T09:50:15.136006Z","shell.execute_reply.started":"2024-10-23T09:49:05.962087Z","shell.execute_reply":"2024-10-23T09:50:15.134859Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-anva7ntv\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-anva7ntv\n  Resolved https://github.com/huggingface/transformers to commit 343c8cb86f2ab6a51e7363ee11f69afb1c9e839e\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.0.dev0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=10025236 sha256=c3c5cda45977216d6ac836631ddeed973a5e656b22f75b4ffb6c2cb2db17bb24\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2ti23ept/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.46.0.dev0\nCollecting qwen-vl-utils\n  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\nCollecting av (from qwen-vl-utils)\n  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from qwen-vl-utils) (21.3)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from qwen-vl-utils) (10.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from qwen-vl-utils) (2.32.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->qwen-vl-utils) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (2024.8.30)\nDownloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\nDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av, qwen-vl-utils\nSuccessfully installed av-13.1.0 qwen-vl-utils-0.0.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 1: Imports\nimport os\nimport torch\nimport gc\nimport time\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport csv\nimport random\n\n# Cell 2: Load Model and Processor\ndef load_model_and_processor(min_pixels=256 * 28 * 28, max_pixels=1280 * 28 * 28):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    try:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n            \"Qwen/Qwen2-VL-2B-Instruct\",\n            torch_dtype=torch.bfloat16 if device.type == \"cuda\" else torch.float32,\n            device_map=\"auto\",\n        )\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None, None, None\n    try:\n        processor = AutoProcessor.from_pretrained(\n            \"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n        )\n        print(f\"Processor loaded successfully with image resizing ({min_pixels} - {max_pixels} pixels).\")\n    except Exception as e:\n        print(f\"Error loading processor: {e}\")\n        return None, None, None\n    model.to(device)\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        model = torch.nn.DataParallel(model)\n    model.eval()\n    return model, processor, device\n\n# Load the model, processor, and device\nmodel, processor, device = load_model_and_processor()\n\n# Cell 3: Define Helper Functions\ndef get_generate_method(model):\n    if isinstance(model, torch.nn.DataParallel):\n        return model.module.generate\n    else:\n        return model.generate\n\ndef process_image(image_path, prompt, processor, model, device):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        image = image.resize((1024, 1024))\n        messages = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image},\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }]\n        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        ).to(device)\n        generate = get_generate_method(model)\n        with torch.no_grad():\n            generated_ids = generate(**inputs, max_new_tokens=128)\n            generated_ids_trimmed = [\n                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n            ]\n            output_text = processor.batch_decode(\n                generated_ids_trimmed,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=False\n            )\n        del inputs, generated_ids, generated_ids_trimmed, image_inputs, video_inputs\n        gc.collect()\n        torch.cuda.empty_cache()\n        return output_text[0], image\n    except Exception as e:\n        print(f\"Error processing image: {e}\")\n        return None, None\n\n# Cell 4: Main Execution\ndef load_valid_ids(csv_file_path):\n    valid_ids = set()\n    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n        csvreader = csv.DictReader(csvfile)\n        for row in csvreader:\n            valid_ids.add(row['id'])\n    return valid_ids\n\ndef main():\n    if model is None or processor is None:\n        print(\"Model or processor not loaded. Exiting.\")\n        return\n\n    prompt = input(\"Enter prompt: \")\n    dataset_dir = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/\"\n\n    if not os.path.isdir(dataset_dir):\n        print(f\"Directory '{dataset_dir}' does not exist. Please check the path.\")\n        return\n\n    # Load valid IDs from the CSV file\n    csv_file_path = \"/kaggle/input/fashion-ds-filtered-datatypes/filtered_article_types.csv\"\n    valid_ids = load_valid_ids(csv_file_path)\n\n    # Get all image files in the directory\n    image_files = [f for f in os.listdir(dataset_dir) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n    \n    # Filter images based on valid IDs (assumes image filenames are formatted as <id>.extension)\n    selected_images = [f for f in image_files if os.path.splitext(f)[0] in valid_ids]\n    \n    # Select up to 300 images\n    if len(selected_images) > 300:\n        selected_images = random.sample(selected_images, 300)\n\n    image_count = 0\n\n    # Open a CSV file to store the results\n    output_csv_file_path = \"/kaggle/working/image_captions.csv\"\n    with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        csvwriter.writerow(['Image File Name', 'Master Category', 'Outfit Piece', 'Pattern', 'Color', 'Material', 'Season', 'Weather', 'Dress code', 'Gender'])\n\n        for image_file in selected_images:\n            image_path = os.path.join(dataset_dir, image_file)\n            image_count += 1\n\n            try:\n                start_time = time.time()\n                caption, resized_image = process_image(image_path, prompt, processor, model, device)\n\n                if caption and resized_image:\n                    elapsed_time = time.time() - start_time\n\n                    plt.figure(figsize=(8, 8))\n                    plt.imshow(resized_image)\n                    plt.axis('off')\n                    plt.title(f\"Image {image_count}: {caption}\\nProcessed in {elapsed_time:.2f} seconds\")\n                    plt.show()\n\n                    print(f\"Caption for image {image_count} ({image_file}): {caption}\")\n                    print(f\"Processed in {elapsed_time:.2f} seconds\\n\")\n\n                    split_caption = [part.strip() for part in caption.split(\",\")]\n                    split_caption = [part.split(\":\")[1].strip() if \":\" in part else \"\" for part in split_caption]\n\n                    while len(split_caption) < 9:\n                        split_caption.append('')\n\n                    csvwriter.writerow([image_file] + split_caption[:9])\n\n                else:\n                    print(f\"Skipping image {image_file}: Could not process.\")\n\n            except Exception as e:\n                print(f\"Error processing image {image_file}: {e}\")\n\n            finally:\n                gc.collect()\n                torch.cuda.empty_cache()\n\n    print(f\"Results saved in {output_csv_file_path}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-23T09:50:15.138693Z","iopub.execute_input":"2024-10-23T09:50:15.139109Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786e5c82d93442adb0b9c4f7afe5d680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e460b90ae354a58a7c88a3a27b761ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3358c6bf379249aab31cda2644051735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5ba348820b46f0940ae0b2d2d91061"}},"metadata":{}}]},{"cell_type":"code","source":"# Run main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}