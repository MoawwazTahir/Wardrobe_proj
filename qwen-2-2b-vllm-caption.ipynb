{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630},{"sourceId":9680276,"sourceType":"datasetVersion","datasetId":5916796}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers\n!pip install qwen-vl-utils","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:37:11.775949Z","iopub.execute_input":"2024-10-22T10:37:11.776720Z","iopub.status.idle":"2024-10-22T10:38:16.127107Z","shell.execute_reply.started":"2024-10-22T10:37:11.776675Z","shell.execute_reply":"2024-10-22T10:38:16.125978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 1: Imports\nimport os\nimport torch\nimport gc\nimport time\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport csv\nimport random\n\n# Cell 2: Load Model and Processor\ndef load_model_and_processor(min_pixels=256 * 28 * 28, max_pixels=1280 * 28 * 28):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    try:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n            \"Qwen/Qwen2-VL-2B-Instruct\",\n            torch_dtype=torch.bfloat16 if device.type == \"cuda\" else torch.float32,\n            device_map=\"auto\",\n        )\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None, None, None\n    try:\n        processor = AutoProcessor.from_pretrained(\n            \"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n        )\n        print(f\"Processor loaded successfully with image resizing ({min_pixels} - {max_pixels} pixels).\")\n    except Exception as e:\n        print(f\"Error loading processor: {e}\")\n        return None, None, None\n    model.to(device)\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        model = torch.nn.DataParallel(model)\n    model.eval()\n    return model, processor, device\n\n# Load the model, processor, and device\nmodel, processor, device = load_model_and_processor()\n\n# Cell 3: Define Helper Functions\ndef get_generate_method(model):\n    if isinstance(model, torch.nn.DataParallel):\n        return model.module.generate\n    else:\n        return model.generate\n\ndef process_image(image_path, prompt, processor, model, device):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        image = image.resize((1024, 1024))\n        messages = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image},\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }]\n        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        ).to(device)\n        generate = get_generate_method(model)\n        with torch.no_grad():\n            generated_ids = generate(**inputs, max_new_tokens=128)\n            generated_ids_trimmed = [\n                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n            ]\n            output_text = processor.batch_decode(\n                generated_ids_trimmed,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=False\n            )\n        del inputs, generated_ids, generated_ids_trimmed, image_inputs, video_inputs\n        gc.collect()\n        torch.cuda.empty_cache()\n        return output_text[0], image\n    except Exception as e:\n        print(f\"Error processing image: {e}\")\n        return None, None\n\n# Cell 4: Main Execution\ndef load_valid_ids(csv_file_path):\n    valid_ids = set()\n    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n        csvreader = csv.DictReader(csvfile)\n        for row in csvreader:\n            valid_ids.add(row['id'])\n    return valid_ids\n\ndef main():\n    if model is None or processor is None:\n        print(\"Model or processor not loaded. Exiting.\")\n        return\n\n    prompt = input(\"Enter prompt: \")\n    dataset_dir = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/\"\n\n    if not os.path.isdir(dataset_dir):\n        print(f\"Directory '{dataset_dir}' does not exist. Please check the path.\")\n        return\n\n    # Load valid IDs from the CSV file\n    csv_file_path = \"/kaggle/input/fashion-ds-filtered-datatypes/filtered_article_types.csv\"\n    valid_ids = load_valid_ids(csv_file_path)\n\n    # Get all image files in the directory\n    image_files = [f for f in os.listdir(dataset_dir) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n    \n    # Filter images based on valid IDs (assumes image filenames are formatted as <id>.extension)\n    selected_images = [f for f in image_files if os.path.splitext(f)[0] in valid_ids]\n    \n    # Select up to 300 images\n    if len(selected_images) > 300:\n        selected_images = random.sample(selected_images, 300)\n\n    image_count = 0\n\n    # Open a CSV file to store the results\n    output_csv_file_path = \"/kaggle/working/image_captions.csv\"\n    with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        csvwriter.writerow(['Image File Name', 'Master Category', 'Outfit Piece', 'Pattern', 'Color', 'Material', 'Season', 'Weather', 'Outfit Type', 'Gender'])\n\n        for image_file in selected_images:\n            image_path = os.path.join(dataset_dir, image_file)\n            image_count += 1\n\n            try:\n                start_time = time.time()\n                caption, resized_image = process_image(image_path, prompt, processor, model, device)\n\n                if caption and resized_image:\n                    elapsed_time = time.time() - start_time\n\n                    plt.figure(figsize=(8, 8))\n                    plt.imshow(resized_image)\n                    plt.axis('off')\n                    plt.title(f\"Image {image_count}: {caption}\\nProcessed in {elapsed_time:.2f} seconds\")\n                    plt.show()\n\n                    print(f\"Caption for image {image_count} ({image_file}): {caption}\")\n                    print(f\"Processed in {elapsed_time:.2f} seconds\\n\")\n\n                    split_caption = [part.strip() for part in caption.split(\",\")]\n                    split_caption = [part.split(\":\")[1].strip() if \":\" in part else \"\" for part in split_caption]\n\n                    while len(split_caption) < 9:\n                        split_caption.append('')\n\n                    csvwriter.writerow([image_file] + split_caption[:9])\n\n                else:\n                    print(f\"Skipping image {image_file}: Could not process.\")\n\n            except Exception as e:\n                print(f\"Error processing image {image_file}: {e}\")\n\n            finally:\n                gc.collect()\n                torch.cuda.empty_cache()\n\n    print(f\"Results saved in {output_csv_file_path}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-22T10:38:16.129225Z","iopub.execute_input":"2024-10-22T10:38:16.129593Z","iopub.status.idle":"2024-10-22T10:39:04.247581Z","shell.execute_reply.started":"2024-10-22T10:38:16.129557Z","shell.execute_reply":"2024-10-22T10:39:04.246616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run main function\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:39:04.250308Z","iopub.execute_input":"2024-10-22T10:39:04.251147Z","iopub.status.idle":"2024-10-22T11:16:13.425634Z","shell.execute_reply.started":"2024-10-22T10:39:04.251111Z","shell.execute_reply":"2024-10-22T11:16:13.424625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}